{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#DATA VISULALIZATION AND STATISTICS\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_csv('all_labels.csv')\n",
        "df.head()\n",
        "\n",
        "list=df['class'].unique()\n",
        "print(list)\n",
        "print(len(list))\n",
        "count_species = {}\n",
        "for species in list:\n",
        "    for x in df['class']:\n",
        "      if(x==species):\n",
        "        count_species[species] = count_species.get(species, 0) + 1\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# create and view data\n",
        "data = pd.DataFrame(count_species.items(),columns=['animals','count'])\n",
        "data\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Sample DataFrame with 45 animals\n",
        "np.random.seed(42)  # For reproducibility\n",
        "\n",
        "# Define colors for bars\n",
        "colors = plt.cm.viridis(np.linspace(0, 1, len(data)))  # Gradient color map\n",
        "\n",
        "# Animals to highlight\n",
        "small_animals = [\n",
        "    \"Aardvark\",\n",
        "    \"Aardwolf\",\n",
        "    \"Agouti\",\n",
        "    \"Coiban_agouti\",\n",
        "    \"Collared_peccary\",\n",
        "    \"Common_opossum\",\n",
        "    \"DikDik\",\n",
        "    \"GazelleGrants\",\n",
        "    \"GazelleThomsons\",\n",
        "    \"HyenaSpotted\",\n",
        "    \"Impala\",\n",
        "    \"Jackal\",\n",
        "    \"Mongoose\",\n",
        "    \"Ocelot\",\n",
        "    \"Paca\",\n",
        "    \"Porcupine\",\n",
        "    \"Red_brocket_deer\",\n",
        "    \"Red_fox\",\n",
        "    \"Red_squirrel\",\n",
        "    \"Rodent\",\n",
        "    \"Serval\",\n",
        "    \"Spiny_rat\",\n",
        "    \"Warthog\",\n",
        "    \"CattleEgret\",\n",
        "    \"GreyBackedFiscal\",\n",
        "    \"GuineaFowl\",\n",
        "    \"SecretaryBird\",\n",
        "    \"SuperbStarling\",\n",
        "    \"WattledStarling\",\n",
        "    \"WhiteHeadBuffaloWeaver\",\n",
        "    \"Tinamou\"\n",
        "]\n",
        "\n",
        "# Create figure and axis\n",
        "fig, ax = plt.subplots(figsize=(10, 15))  # Increase height to fit all labels\n",
        "\n",
        "# Horizontal bar plot\n",
        "ax.barh(data[\"animals\"], data[\"count\"], color=colors)\n",
        "\n",
        "# Labels and title\n",
        "plt.xlabel(\"Count\")\n",
        "plt.ylabel(\"Animals\")\n",
        "plt.title(\"Animal Count Visualization\")\n",
        "\n",
        "# Highlight specific labels in bold red\n",
        "for label in ax.get_yticklabels():\n",
        "    if label.get_text() in small_animals:\n",
        "        label.set_color(\"red\")\n",
        "        label.set_fontweight(\"bold\")\n",
        "\n",
        "# Improve layout\n",
        "plt.gca().invert_yaxis()  # Invert y-axis so highest values appear on top\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jlmi4I36Zvyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#IMAGES DOWNLOADING FROM SNAPSHOT\n",
        "\n",
        "import json\n",
        "from collections import Counter\n",
        "\n",
        "# Path to the metadata file in your drive\n",
        "metadata_path = '/content/drive/MyDrive/snapshot_safari_2024_metadata.json'\n",
        "\n",
        "# Load the metadata file\n",
        "with open(metadata_path, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Count species\n",
        "species_map = {cat['id']: cat['name'] for cat in data['categories']}\n",
        "species_count = Counter(species_map[ann['category_id']] for ann in data['annotations'])\n",
        "\n",
        "# Print counts of all species\n",
        "for species, count in species_count.items():\n",
        "    print(f\"{species}: {count}\")\n",
        "\n",
        "# Specifically for Impala\n",
        "print(\"\\nTotal Impala Images:\", species_count.get('impala', 0))\n"
      ],
      "metadata": {
        "id": "W-vcTPKVZgNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#IMAGES DOWNLOADING FROM WCS\n",
        "\n",
        "import os\n",
        "import json\n",
        "import csv\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "# Define species of interest\n",
        "species_of_interest = {\n",
        "    \"proteles cristata\", \"ceratotherium simum\", \"dicerorhinus sumatrensis\", \"diceros bicornis\",\n",
        "    \"lepus saxatilis\", \"kobus ellipsiprymnus\", \"chlorocebus pygerythrus\", \"tinamus sp\", \"tinamus tao\",\n",
        "    \"tinamus major\", \"tinamus guttatus\", \"crypturellus erythropus\", \"odocoileus virginianus\",\n",
        "    \"didelphis marsupialis\", \"didelphis sp\", \"didelphis virginiana\", \"didelphis pernigra\",\n",
        "    \"didelphis imperfecta\", \"proechimys sp\", \"proechimys\", \"sciurus spadiceus\", \"sciurus ignitus\",\n",
        "    \"sciurus igniventris\", \"microsciurus flaviventer\", \"sciurus gramatensis\", \"funisciurus carruthersi\",\n",
        "    \"heliosciurus rufobrachium\", \"heliosciurus ruwenzorii\", \"sciurus deppei\", \"sundasciurus hippurus\",\n",
        "    \"callosciurus notatus\", \"callosciurus erythraeus\", \"sciurus sp\", \"funisciurus pyrropus\",\n",
        "    \"tragelaphus scriptus\", \"nasua narica\", \"nasua nasua\", \"leopardus pardalis\", \"dasyprocta punctata\",\n",
        "    \"dasyprocta fuliginosa\", \"dasyprocta leporina\", \"sus scrofa\", \"cuniculus paca\", \"cuniculus taczanowskii\",\n",
        "    \"mazama americana\", \"pecari tajacu\"\n",
        "}\n",
        "\n",
        "# Load JSON data\n",
        "json_file = \"wcs_20220205_bboxes_with_classes.json\"\n",
        "with open(json_file, \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Base download directory\n",
        "download_dir = r\"D:\\HPE\\Dataset\\WCS_Images\"\n",
        "os.makedirs(download_dir, exist_ok=True)\n",
        "\n",
        "# CSV file to store mappings\n",
        "csv_file = \"image_download_mapping.csv\"\n",
        "\n",
        "# Base URL for downloads\n",
        "base_url = \"https://storage.googleapis.com/public-datasets-lila/wcs-unzipped/\"\n",
        "\n",
        "# Load existing mappings to resume downloads\n",
        "downloaded_images = set()\n",
        "species_count = defaultdict(int)\n",
        "\n",
        "if os.path.exists(csv_file):\n",
        "    with open(csv_file, \"r\") as f:\n",
        "        reader = csv.reader(f)\n",
        "        next(reader)  # Skip header\n",
        "        for row in reader:\n",
        "            downloaded_images.add(row[0])  # Original file name\n",
        "            species_name = row[1].split(os.sep)[-2]  # Extract species name\n",
        "            species_count[species_name] += 1\n",
        "\n",
        "# Store new mappings\n",
        "mapping_data = []\n",
        "\n",
        "# Process images\n",
        "for image in tqdm(data[\"images\"], desc=\"Downloading Images\"):\n",
        "    file_name = image[\"file_name\"]  # Example: \"humans/0410/1262.jpg\"\n",
        "\n",
        "    if file_name in downloaded_images:\n",
        "        continue  # Skip already downloaded files\n",
        "\n",
        "    image_id = image[\"id\"]\n",
        "\n",
        "    # Get category_id from annotations\n",
        "    category_id = next((ann[\"category_id\"] for ann in data[\"annotations\"] if ann[\"image_id\"] == image_id), None)\n",
        "    if category_id is None:\n",
        "        continue\n",
        "\n",
        "    # Get species name from categories\n",
        "    species_name = next((cat[\"name\"] for cat in data[\"categories\"] if cat[\"id\"] == category_id), \"unknown\")\n",
        "\n",
        "    # If species is of interest and limit is not reached\n",
        "    if species_name.lower() in species_of_interest and species_count[species_name] < 2500:\n",
        "        image_url = base_url + file_name  # Construct full URL\n",
        "\n",
        "        # Format filename to avoid conflicts\n",
        "        formatted_name = file_name.replace(\"/\", \"_\")  # Example: \"humans_0410_1262.jpg\"\n",
        "        save_path = os.path.join(download_dir, formatted_name)\n",
        "\n",
        "        try:\n",
        "            response = requests.get(image_url, stream=True, timeout=10)\n",
        "            if response.status_code == 200:\n",
        "                with open(save_path, \"wb\") as img_file:\n",
        "                    for chunk in response.iter_content(1024):\n",
        "                        img_file.write(chunk)\n",
        "\n",
        "                # Update tracking\n",
        "                species_count[species_name] += 1\n",
        "                mapping_data.append([file_name, save_path])\n",
        "\n",
        "                # Append to CSV immediately to save progress\n",
        "                with open(csv_file, \"a\", newline=\"\") as f:\n",
        "                    writer = csv.writer(f)\n",
        "                    writer.writerow([file_name, save_path])\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to download {image_url}: {e}\")\n",
        "\n",
        "print(f\"Download completed. Mapping saved in {csv_file}\")\n"
      ],
      "metadata": {
        "id": "SDqQg7vuVNHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jBFloFSWZ7S"
      },
      "outputs": [],
      "source": [
        "#AUGMENTATION\n",
        "import albumentations as A\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "\n",
        "\n",
        "meta_df = pd.read_csv('meta_data_13.csv')\n",
        "output_dir = 'C:/Users/vedit/Pictures/Data_Extracted/images'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "augmented_metadata = []\n",
        "\n",
        "\n",
        "standard_transform = lambda include_noise: A.Compose([\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.2),\n",
        "    A.RandomBrightnessContrast(p=0.5),\n",
        "    A.RandomGamma(p=0.2),\n",
        "    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),\n",
        "    A.Rotate(limit=15, p=0.5),\n",
        "    A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), p=0.3),\n",
        "    A.GaussNoise(var_limit=(5.0, 10.0), p=0.4) if include_noise else A.NoOp()\n",
        "], bbox_params=A.BboxParams(format='yolo', label_fields=['labels']))\n",
        "\n",
        "mild_transform = A.Compose([\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.RandomBrightnessContrast(p=0.3),\n",
        "    A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), p=0.3),\n",
        "], bbox_params=A.BboxParams(format='yolo', label_fields=['labels']))\n",
        "\n",
        "\n",
        "species_counts = meta_df['filename'].drop_duplicates().groupby(meta_df['class']).count()\n",
        "species_to_augment = species_counts[species_counts < 1600].index.tolist()\n",
        "\n",
        "def augment_image(image_path, bboxes, labels, species, count):\n",
        "    try:\n",
        "        image = cv2.imread(image_path)\n",
        "        if image is None:\n",
        "            print(f\"Failed to load {image_path}\")\n",
        "            return\n",
        "\n",
        "        height, width = image.shape[:2]\n",
        "        use_mild = any(w * h < 0.02 for _, _, w, h in bboxes)\n",
        "        use_noise = any(w * h > 0.1 for _, _, w, h in bboxes)\n",
        "\n",
        "        transform = mild_transform if use_mild else standard_transform(include_noise=use_noise)\n",
        "        augmented = transform(image=image, bboxes=bboxes, labels=labels)\n",
        "        augmented_image = augmented['image']\n",
        "        new_filename = f\"{species}_aug_{count}_{os.path.basename(image_path)}\"\n",
        "        new_path = os.path.join(output_dir, new_filename)\n",
        "        cv2.imwrite(new_path, augmented_image)\n",
        "\n",
        "        for bbox, label in zip(augmented['bboxes'], augmented['labels']):\n",
        "            x_c, y_c, w, h = bbox\n",
        "            if x_c < 0 or y_c < 0 or x_c > 1 or y_c > 1 or w * h < 0.001:\n",
        "                continue\n",
        "\n",
        "            augmented_metadata.append({\n",
        "                'filename': new_filename,\n",
        "                'width': width,\n",
        "                'height': height,\n",
        "                'class': label,\n",
        "                'x_center': x_c,\n",
        "                'y_center': y_c,\n",
        "                'bbox_width': w,\n",
        "                'bbox_height': h\n",
        "            })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {image_path}: {e}\")\n",
        "\n",
        "\n",
        "for species in species_to_augment:\n",
        "    species_data = meta_df[meta_df['class'] == species]\n",
        "    unique_images = species_data['filename'].unique()\n",
        "    current_count = len(unique_images)\n",
        "    augment_count = 0\n",
        "\n",
        "    while current_count + augment_count < 1600:\n",
        "        image_name = random.choice(unique_images)\n",
        "        rows = species_data[species_data['filename'] == image_name]\n",
        "        boxes = rows[['x_center', 'y_center', 'bbox_width', 'bbox_height']].values.tolist()\n",
        "        labels = rows['class'].tolist()\n",
        "\n",
        "        if not any(w * h >= 0.001 for _, _, w, h in boxes):\n",
        "            continue\n",
        "\n",
        "        augment_count += 1\n",
        "        augment_image(image_name, boxes, labels, species, augment_count)\n",
        "\n",
        "\n",
        "pd.DataFrame(augmented_metadata).to_csv('augmented_meta_data_13.csv', index=False)\n",
        "print(\"✅ Done. Saved with conditional Gauss noise for large objects.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# === CONFIG ===\n",
        "csv_path = 'metadata15.csv'  # Path to CSV\n",
        "label_output_dir = 'labels'         # Output dir for .txt label files\n",
        "yaml_output_path = 'data.yaml'           # Path for YAML config\n",
        "train_images_path = 'images/train'       # Update as per your YOLO folder structure\n",
        "val_images_path = 'images/val'           # Optional: if splitting dataset\n",
        "os.makedirs(label_output_dir, exist_ok=True)\n",
        "\n",
        "# === LOAD CSV ===\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# === Encode class names to class IDs ===\n",
        "class_list = sorted(df['class'].unique())\n",
        "class_to_id = {cls_name: idx for idx, cls_name in enumerate(class_list)}\n",
        "\n",
        "# === Write YOLO label files ===\n",
        "for filename, group in df.groupby('filename'):\n",
        "    txt_filename = os.path.splitext(filename)[0] + '.txt'\n",
        "    label_file_path = os.path.join(label_output_dir, txt_filename)\n",
        "\n",
        "    with open(label_file_path, 'w') as f:\n",
        "        for _, row in group.iterrows():\n",
        "            class_id = class_to_id[row['class']]\n",
        "            f.write(f\"{class_id} {row['x_center']} {row['y_center']} {row['bbox_width']} {row['bbox_height']}\\n\")\n",
        "\n",
        "# === Write data.yaml ===\n",
        "yaml_lines = [\n",
        "    f\"path: .\",\n",
        "    f\"train: {train_images_path}\",\n",
        "    f\"val: {val_images_path}\",\n",
        "    f\"nc: {len(class_list)}\",\n",
        "    f\"names: {class_list}\"\n",
        "]\n",
        "\n",
        "with open(yaml_output_path, 'w') as f:\n",
        "    f.write('\\n'.join(yaml_lines))\n",
        "\n",
        "print(f\"✅ YOLO labels written to: {label_output_dir}\")\n",
        "print(f\"✅ data.yaml written to: {yaml_output_path}\")\n",
        "print(\"📦 Class map:\", class_to_id)\n"
      ],
      "metadata": {
        "id": "UTSZbemnlCGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the updated metadata CSV\n",
        "csv_file = \"updated_meta_data_12.csv\"\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "# Create a list to hold the new rows for meta_data_13\n",
        "new_rows = []\n",
        "\n",
        "# Loop through each row in the original CSV\n",
        "for _, row in df.iterrows():\n",
        "    filename = row[\"filename\"]\n",
        "    width = row[\"width\"]\n",
        "    height = row[\"height\"]\n",
        "    class_name = row[\"class\"]\n",
        "\n",
        "    # Normalized values\n",
        "    x_min = row[\"x_min\"]\n",
        "    y_min = row[\"y_min\"]\n",
        "    w = row[\"w\"]\n",
        "    h = row[\"h\"]\n",
        "\n",
        "    # Calculate YOLO format values\n",
        "    x_center = round(x_min + (w / 2), 4)\n",
        "    y_center = round(y_min + (h / 2), 4)\n",
        "    bbox_width = round(w, 4)\n",
        "    bbox_height = round(h, 4)\n",
        "\n",
        "    # Append the new row to the list\n",
        "    new_rows.append({\n",
        "        \"filename\": filename,\n",
        "        \"width\": width,\n",
        "        \"height\": height,\n",
        "        \"class\": class_name,\n",
        "        \"x_center\": x_center,\n",
        "        \"y_center\": y_center,\n",
        "        \"bbox_width\": bbox_width,\n",
        "        \"bbox_height\": bbox_height\n",
        "    })\n",
        "\n",
        "# Create a DataFrame for the new CSV\n",
        "new_df = pd.DataFrame(new_rows)\n",
        "\n",
        "# Save the new DataFrame to a new CSV file\n",
        "new_df.to_csv(\"meta_data_13.csv\", index=False)\n",
        "\n",
        "print(\"meta_data_13.csv has been created.\")\n"
      ],
      "metadata": {
        "id": "uUNm20WclCB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from collections import defaultdict\n",
        "from os.path import basename, splitext\n",
        "\n",
        "# Load CSV\n",
        "csv_path = 'meta_data_9.csv'\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Load JSON\n",
        "json_path = 'snapshot_safari_2024_lila_mdv5a.json'\n",
        "with open(json_path, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Step 1: Prepare JSON bbox mapping using filename (without extension)\n",
        "json_bboxes = defaultdict(list)\n",
        "for item in data['images']:\n",
        "    fname = splitext(basename(item['file']))[0]  # remove extension\n",
        "    for det in item['detections']:\n",
        "        bbox = det['bbox']  # [x, y, width, height]\n",
        "        json_bboxes[fname].append(bbox)\n",
        "\n",
        "# Step 2: Prepare updated DataFrame\n",
        "updated_rows = []\n",
        "\n",
        "# Group by filename (without extension)\n",
        "df['base_filename'] = df['filename'].apply(lambda x: splitext(basename(x))[0])\n",
        "filename_groups = df.groupby('base_filename')\n",
        "\n",
        "for base_fname, group in filename_groups:\n",
        "    existing_rows = group.copy()\n",
        "    json_entries = json_bboxes.get(base_fname, [])\n",
        "\n",
        "    for i, bbox in enumerate(json_entries):\n",
        "        if i < len(existing_rows):\n",
        "            # Update existing row\n",
        "            row = existing_rows.iloc[i].copy()\n",
        "            row['x_center'], row['y_center'], row['bbox_width'], row['bbox_height'] = bbox\n",
        "        else:\n",
        "            # Add new row using data from the first row\n",
        "            row = existing_rows.iloc[0].copy()\n",
        "            row['x_center'], row['y_center'], row['bbox_width'], row['bbox_height'] = bbox\n",
        "        updated_rows.append(row)\n",
        "\n",
        "    # Add remaining rows if any JSON entry is missing\n",
        "    if len(json_entries) < len(existing_rows):\n",
        "        for i in range(len(json_entries), len(existing_rows)):\n",
        "            updated_rows.append(existing_rows.iloc[i])\n",
        "\n",
        "# Create new DataFrame\n",
        "updated_df = pd.DataFrame(updated_rows)\n",
        "updated_df.drop(columns=['base_filename'], inplace=True)\n",
        "\n",
        "# Save updated CSV\n",
        "updated_df.to_csv('meta_data_10.csv', index=False)\n",
        "print(\"✅ Updated all bbox values to 'meta_data_9.csv' including additional entries when needed.\")\n"
      ],
      "metadata": {
        "id": "iNlKYaGglB17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# === CONFIG ===\n",
        "csv_path = 'augmented_meta_data_13.csv'         # Your metadata CSV file\n",
        "image_base_path = 'C:/Users/vedit/Pictures/augmented'   # Replace with path to your augmented images\n",
        "num_samples = 100                     # Number of random images to check\n",
        "\n",
        "# === STEP 1: Load and Filter ===\n",
        "df = pd.read_csv(csv_path)\n",
        "df_aug = df[df['filename'].str.contains('animals', case=False, na=False)]\n",
        "sampled_df = df_aug.sample(n=min(num_samples, len(df_aug)), random_state=42).reset_index(drop=True)\n",
        "\n",
        "# === STEP 2: Visualize YOLO bboxes ===\n",
        "def show_image_with_bboxes(img_path, bboxes, labels):\n",
        "    image = cv2.imread(img_path)\n",
        "    if image is None:\n",
        "        print(f\"⚠️ Failed to load image: {img_path}\")\n",
        "        return\n",
        "\n",
        "    height, width = image.shape[:2]\n",
        "\n",
        "    for bbox, label in zip(bboxes, labels):\n",
        "        x_center, y_center, w, h = bbox\n",
        "        x_min = int((x_center - w / 2) * width)\n",
        "        y_min = int((y_center - h / 2) * height)\n",
        "        x_max = int((x_center + w / 2) * width)\n",
        "        y_max = int((y_center + h / 2) * height)\n",
        "\n",
        "        # Draw bounding box\n",
        "        color = (0, 255, 0)\n",
        "        cv2.rectangle(image, (x_min, y_min), (x_max, y_max), color, 2)\n",
        "        cv2.putText(image, str(label), (x_min, y_min - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 1)\n",
        "\n",
        "    # Show image\n",
        "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# === STEP 3: Loop through sampled images ===\n",
        "for idx, row in sampled_df.iterrows():\n",
        "    img_path = os.path.join(image_base_path, row['filename'])\n",
        "\n",
        "    # Ensure bbox is in YOLO format\n",
        "    bbox = [row['x_center'], row['y_center'], row['bbox_width'], row['bbox_height']]\n",
        "    label = row['class_id'] if 'class_id' in row else row['label'] if 'label' in row else 0\n",
        "\n",
        "    try:\n",
        "        show_image_with_bboxes(img_path, [bbox], [label])\n",
        "    except Exception as e:\n",
        "        print(f\"Error showing {row['filename']}: {e}\")\n"
      ],
      "metadata": {
        "id": "1nteqbawlBzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#YOLO416\n",
        "import os\n",
        "from ultralytics import YOLO\n",
        "import pandas as pd\n",
        "import yaml\n",
        "\n",
        "data_yaml_path = 'data.yaml'\n",
        "save_dir = 'results'\n",
        "\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "model = YOLO(\"yolov8n.pt\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train_results = model.train(\n",
        "        data=data_yaml_path,\n",
        "        epochs=50,\n",
        "        batch=4,\n",
        "        imgsz=416,\n",
        "        device=0,\n",
        "        project=save_dir,\n",
        "        name=\"yolov8_training\",\n",
        "        save_period=1,\n",
        "        save_txt=False,\n",
        "        save_json=False,\n",
        "        plots=True,\n",
        "        workers=2,\n",
        "        val=True\n",
        "    )\n",
        "\n",
        "\n",
        "    results_yaml_path = os.path.join(save_dir, \"yolov8_training\", \"results.yaml\")\n",
        "\n",
        "    if os.path.exists(results_yaml_path):\n",
        "        with open(results_yaml_path, 'r') as f:\n",
        "            results_data = yaml.safe_load(f)\n",
        "\n",
        "        metrics_df = pd.DataFrame([results_data])\n",
        "        metrics_df.to_csv(os.path.join(save_dir, \"training_metrics.csv\"), index=False)\n",
        "        print(f\"Training metrics saved to {os.path.join(save_dir, 'training_metrics.csv')}\")\n",
        "    else:\n",
        "        print(\"Warning: results.yaml not found.\")\n",
        "\n",
        "    print(f\"Training completed! Check your results in {save_dir}\")\n"
      ],
      "metadata": {
        "id": "qJ6xiO7HXHdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#YOLO640\n",
        "import os\n",
        "from ultralytics import YOLO\n",
        "import pandas as pd\n",
        "data_yaml_path = 'data.yaml'\n",
        "training_name = 'runs/detect'\n",
        "training_path = training_name\n",
        "\n",
        "\n",
        "model_path = 'runs/detect/train2_continue/weights/last.pt'\n",
        "os.makedirs(training_path, exist_ok=True)\n",
        "\n",
        "model = YOLO(model_path)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train_results = model.train(\n",
        "        resume=True,\n",
        "        epochs=25,\n",
        "        augment=False,\n",
        "        batch=4,\n",
        "        lr0=0.000201,\n",
        "        optimizer='SGD',\n",
        "        imgsz=640,\n",
        "        device=0,\n",
        "        save_period=1,\n",
        "        save_txt=False,\n",
        "        save_json=False,\n",
        "        plots=True,\n",
        "        workers=2,\n",
        "        project='runs/detect',\n",
        "        name='train2_continue',\n",
        "        val=True\n",
        "    )\n",
        "\n",
        "    results_csv_path = os.path.join(training_path, \"results.csv\")\n",
        "    print(f\"Training resumed and completed! Check your results in {training_path}\")\n"
      ],
      "metadata": {
        "id": "n5ZO3ST_XR8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#YOLO960\n",
        "import os\n",
        "from ultralytics import YOLO\n",
        "import pandas as pd\n",
        "\n",
        "data_yaml_path = 'data.yaml'\n",
        "\n",
        "training_name = 'runs/detect'\n",
        "training_path = training_name\n",
        "\n",
        "model_path = 'runs/detect/train3_continue3/weights/last.pt'\n",
        "\n",
        "\n",
        "os.makedirs(training_path, exist_ok=True)\n",
        "\n",
        "\n",
        "model = YOLO(model_path)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train_results = model.train(\n",
        "        resume=True,\n",
        "        epochs=50,\n",
        "        batch=4,\n",
        "        lr0=0.0005,\n",
        "        optimizer='SGD',\n",
        "        imgsz=960,\n",
        "        device=0,\n",
        "        save_period=1,\n",
        "        save_txt=False,\n",
        "        save_json=False,\n",
        "        plots=True,\n",
        "        workers=2,\n",
        "        project='runs/detect',\n",
        "        name='train3_continue',\n",
        "        val=True,\n",
        "        mosaic=0.0,\n",
        "        mixup=0.0,\n",
        "        copy_paste=0.0,\n",
        "        degrees=0.0,\n",
        "        shear=0.0,\n",
        "        perspective=0.0,\n",
        "        flipud=0.0,\n",
        "    )\n",
        "\n",
        "    results_csv_path = os.path.join(training_path, \"results.csv\")\n",
        "    print(f\"Training resumed and completed! Check your results in {training_path}\")\n"
      ],
      "metadata": {
        "id": "CFnE-BPIXTYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#STREAMLIT FRONTEND\n",
        "import streamlit as st\n",
        "from PIL import Image\n",
        "import torch\n",
        "from ultralytics import YOLO\n",
        "import numpy as np\n",
        "\n",
        "model = YOLO('/Users/monishp/Downloads/runs/train/experiment_1/weights/best.pt')\n",
        "\n",
        "st.title(\"Animal Classification Detection App\")\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Choose an image...\", type=[\"jpg\", \"jpeg\", \"png\",\"JPG\"])\n",
        "if uploaded_file is not None:\n",
        "    image = Image.open(uploaded_file).convert('RGB')\n",
        "    st.image(image, caption='Uploaded Image', use_column_width=True)\n",
        "\n",
        "    # Run YOLO inference\n",
        "    results = model(image)\n",
        "\n",
        "    # Show the image with detections\n",
        "    res_plotted = results[0].plot()  # Plots bounding boxes on image\n",
        "    st.image(res_plotted, caption='Detected Image', use_column_width=True)"
      ],
      "metadata": {
        "id": "DBtbFbwaXqFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h7B5UjfuXscM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}